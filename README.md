# PySpark + PostgreSQL Playground

A self-contained, reproducible Data Engineering sandbox built using **Docker Compose** â€” combining:
- ğŸ§® **PySpark** for distributed data processing
- ğŸ˜ **PostgreSQL** for SQL practice
- ğŸ§° **pgAdmin** for database visualization
- ğŸ’» **JupyterLab** for interactive PySpark notebooks

Ideal for:
- Practicing **LeetCode SQL** questions using Spark
- Learning **PySpark JDBC connections**
- Building **data pipelines** locally before deploying to cloud
- Teaching or demoing **Data Engineering fundamentals**

---

## ğŸš€ Features
- One-command startup using `docker compose up -d`
- Shared environment variables via `.env` file
- Persistent PostgreSQL storage via Docker volumes
- Configured JupyterLab workspace (PySpark pre-installed)
- Integrated pgAdmin for DB management
- Secure defaults â€” secrets isolated in `.env`

---

## ğŸ§° Stack Overview
| Component | Purpose |
|------------|----------|
| PySpark (JupyterLab) | Run Spark code interactively |
| PostgreSQL | Backend relational database |
| pgAdmin | Browser-based SQL & DB management |
| Docker Compose | Orchestrates the environment |

## Project Layout

leetcode-pyspark/<br>
â”œâ”€ docker-compose.yml<br>
â”œâ”€ .env.example<br>
â”œâ”€ .gitignore<br>
â”œâ”€ README.md<br>
â”œâ”€ libs/                     # optional: place JDBC jar(s) here<br>
â”œâ”€ workspace/<br>
â”‚  â”œâ”€ Dockerfile<br>
â”‚  â”œâ”€ requirements.txt<br>
â”‚  â”œâ”€ seed_data.py<br>
â”‚  â””â”€ notebooks/             # your Jupyter notebooks<br>
â””â”€ .devcontainer/<br>
   â””â”€ devcontainer.json<br>

## Step-by-step setup (for a new user)

### 1) Clone the repo
```bash
git clone <repo-url>
cd pyspark-postgres-playground   # or your repo folder name
```

### 2) Create .env from the template (do NOT commit .env)
```
cp .env.example .env
# Edit .env if you want custom credentials
```






